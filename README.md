# DocVQA Semantic Layout Graph Pipeline

## üéØ M·ª•c ti√™u d·ª± √°n

X√¢y d·ª±ng pipeline sinh d·ªØ li·ªáu DocVQA v·ªõi **Semantic Layout Graph** ƒë·ªÉ t·∫°o c√¢u h·ªèi/ƒë√°p √°n ki·ªÉu **cross-element**:

- **Text ‚Üî Table**: "Theo b·∫£ng gi√°, s·∫£n ph·∫©m n√†o trong ti√™u ƒë·ªÅ c√≥ gi√° cao nh·∫•t?"
- **Figure ‚Üî Caption**: "Bi·ªÉu ƒë·ªì n√†o m√¥ t·∫£ xu h∆∞·ªõng ƒë∆∞·ª£c nh·∫Øc trong ƒëo·∫°n text?"
- **Form ‚Üî Text**: "T·ªïng ti·ªÅn trong form c√≥ kh·ªõp v·ªõi s·ªë ti·ªÅn trong h·ª£p ƒë·ªìng kh√¥ng?"
- **Text ‚Üî Text**: "So s√°nh th√¥ng tin ng∆∞·ªùi g·ª≠i v√† ng∆∞·ªùi nh·∫≠n trong th∆∞"

---

## üìã 6 Deliverables

| # | Deliverable                           | M√¥ t·∫£                                      | Status          |
| - | ------------------------------------- | -------------------------------------------- | --------------- |
| 1 | **Schema Design**               | JSON schema cho OCR + Layout + Graph + QA    | ‚úÖ Ho√†n th√†nh |
| 2 | **OCR + Preprocessing**         | PaddleOCR + text grouping + normalization    | ‚úÖ Ho√†n th√†nh |
| 3 | **Layout Classification**       | Ph√¢n lo·∫°i regions (Text/Table/Form/Figure) | üîÑ Ti·∫øp theo   |
| 4 | **Semantic Graph Construction** | X√¢y graph G=(V,E) v·ªõi spatial relations    | ‚è≥ Ch·ªù         |
| 5 | **Hybrid QA Generation**        | Rule-based + LLM cross-element               | ‚è≥ Ch·ªù         |
| 6 | **Dataset Packaging**           | Export JSONL/CSV + evidence pointers         | ‚è≥ Ch·ªù         |
| 7 | **Evaluation Framework**        | Coverage, answerability, consistency         | ‚è≥ Ch·ªù         |

---

## üìÅ C·∫•u tr√∫c th∆∞ m·ª•c d·ª± √°n

```
code/
‚îú‚îÄ‚îÄ README.md                          # ‚Üê File n√†y (t·ªïng quan d·ª± √°n)
‚îú‚îÄ‚îÄ schema/
‚îÇ   ‚îú‚îÄ‚îÄ sample_docvqa_graph.json       # ‚úÖ DELIVERABLE 1: Sample data ƒë·∫ßy ƒë·ªß
‚îÇ   ‚îî‚îÄ‚îÄ schema_definition.md           # ‚úÖ DELIVERABLE 1: Chi ti·∫øt schema
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ ocr/                           # ‚úÖ DELIVERABLE 2: OCR Pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ paddle_ocr_engine.py       # PaddleOCR wrapper + preprocessing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text_grouping.py           # Token grouping (lines/blocks)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_normalizer.py         # Text normalization + fuzzy matching
‚îÇ   ‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_downloader.py         # Download DocVQA dataset
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ constant.py                # Constants
‚îÇ       ‚îî‚îÄ‚îÄ __pycache__/
‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îú‚îÄ‚îÄ 1_Download_data.ipynb          # Notebook t·∫£i data
‚îÇ   ‚îî‚îÄ‚îÄ 2_OCR_Extraction.ipynb         # ‚úÖ DELIVERABLE 2: OCR demo notebook
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îú‚îÄ‚îÄ DocVQA/                        # Raw DocVQA data
‚îÇ   ‚îú‚îÄ‚îÄ DocVQA_Images/                 # Images (train/val/test)
‚îÇ   ‚îú‚îÄ‚îÄ DocVQA_Labels/                 # Labels CSV
‚îÇ   ‚îú‚îÄ‚îÄ DocVQA_OCR/                    # OCR results (s·∫Ω t·∫°o)
‚îÇ   ‚îî‚îÄ‚îÄ DocVQA_raw/                    # Original data
‚îî‚îÄ‚îÄ requirements.txt                   # ‚úÖ Updated v·ªõi PaddleOCR dependencies
```

---

## ‚úÖ DELIVERABLE 2: OCR & Preprocessing (HO√ÄN TH√ÄNH)

### T·ªïng quan

ƒê√£ x√¢y d·ª±ng pipeline OCR extraction ho√†n ch·ªânh v·ªõi PaddleOCR:

- **Image Preprocessing**: resize, denoise, contrast enhancement, deskew
- **OCR Extraction**: token-level text v·ªõi bbox + confidence
- **Token Grouping**: spatial heuristics ƒë·ªÉ group th√†nh lines/blocks
- **Text Normalization**: chu·∫©n h√≥a text cho answer matching

### Files ƒë√£ t·∫°o

#### 1. [`src/ocr/paddle_ocr_engine.py`](src/ocr/paddle_ocr_engine.py)

**M·ª•c ƒë√≠ch**: PaddleOCR wrapper v·ªõi preprocessing pipeline

**Components ch√≠nh**:

**A. ImagePreprocessor Class**

```python
preprocessor = ImagePreprocessor(
    target_dpi=300,        # Target resolution
    denoise=True,          # Non-Local Means denoising
    enhance_contrast=True, # CLAHE contrast enhancement
    deskew=True           # Rotation correction
)
preprocessed_image = preprocessor.preprocess(image)
```

**Preprocessing steps**:

1. **Resize**: Normalize resolution, limit max size (3000px) ƒë·ªÉ tr√°nh OOM
2. **Denoise**: `cv2.fastNlMeansDenoising()` - t·ªët cho ·∫£nh ch·ª•p + scanned docs
3. **Deskew**: Detect rotation angle b·∫±ng `cv2.minAreaRect()`, rotate n·∫øu > 0.5¬∞
4. **Contrast Enhancement**: CLAHE (Contrast Limited Adaptive Histogram Equalization)

**L√Ω do preprocessing**:

- **Handwriting**: Denoising gi·∫£m noise t·ª´ ·∫£nh ch·ª•p
- **Photograph**: Contrast enhancement c·∫£i thi·ªán text visibility
- **Scanned docs**: Deskew correct g√≥c qu√©t l·ªách

**B. PaddleOCREngine Class**

```python
engine = PaddleOCREngine(
    lang='en',           # Language code
    use_angle_cls=True,  # Text direction detection
    use_gpu=False,       # GPU support
    show_log=False
)

ocr_data = engine.run_ocr(image_path, preprocess=True)
```

**Output format** (theo schema):

```python
{
    "engine": "paddleocr",
    "version": "2.7.0",
    "language": "en",
    "tokens": [
        {
            "token_id": 0,
            "text": "INVOICE",
            "bbox": [120, 80, 380, 140],  # [x1, y1, x2, y2]
            "confidence": 0.98,
            "font_size": 48,               # Estimated from height
            "is_bold": False
        }
    ],
    "extraction_time_ms": 1250
}
```

**T·∫°i sao PaddleOCR?**

- ‚úÖ Multi-language support (80+ languages)
- ‚úÖ Handwriting support (better than Tesseract)
- ‚úÖ Lightweight (no GPU required)
- ‚úÖ Rotation detection built-in
- ‚úÖ Good for document images (forms, invoices, receipts)

#### 2. [`src/ocr/text_grouping.py`](src/ocr/text_grouping.py)

**M·ª•c ƒë√≠ch**: Group OCR tokens th√†nh hierarchical structures

**Components ch√≠nh**:

**TextGrouper Class**

```python
grouper = TextGrouper(
    line_height_threshold=1.5,   # Max height ratio for same line
    line_gap_threshold=2.0,       # Max vertical gap (√ó avg height)
    block_gap_threshold=3.0       # Min gap between blocks
)

result = grouper.group_tokens(tokens)
# Returns: {'lines': [...], 'blocks': [...]}
```

**Algorithm: Line Grouping**

1. Sort tokens by Y-coordinate (top‚Üíbottom), then X (left‚Üíright)
2. For each token, check if belongs to current line:
   - **Vertical alignment**: y_center within line's y-range
   - **Height similarity**: ratio < 1.5x
   - **Horizontal continuity**: gap < 2√ó avg char width
3. If not, start new line

**Algorithm: Block Grouping**

1. Sort lines by Y-coordinate
2. Group consecutive lines with:
   - **Small vertical gap**: < 3√ó avg line height
   - **Horizontal alignment**: overlap > 30% OR aligned edges

**Output structure**:

```python
{
    "lines": [
        {
            "line_id": 0,
            "tokens": [...],          # Original tokens
            "token_ids": [1, 2, 3],  # Token IDs
            "bbox": [x1, y1, x2, y2],
            "text": "Date: 2024-03-15",  # Concatenated
            "confidence": 0.96,       # Average
            "num_tokens": 3
        }
    ],
    "blocks": [
        {
            "block_id": 0,
            "lines": [0, 1, 2],      # Line IDs
            "bbox": [x1, y1, x2, y2],
            "text": "Date: 2024-03-15\nInvoice: INV-001",
            "confidence": 0.95,
            "num_lines": 3,
            "num_tokens": 8
        }
    ]
}
```

**T·∫°i sao c·∫ßn grouping?**

- ‚úÖ Preserve reading order (top‚Üíbottom, left‚Üíright)
- ‚úÖ Identify paragraphs/sections for layout analysis
- ‚úÖ Multi-column detection (newspaper, reports)
- ‚úÖ Better context for QA generation (question about "this paragraph")

#### 3. [`src/ocr/text_normalizer.py`](src/ocr/text_normalizer.py)

**M·ª•c ƒë√≠ch**: Normalize text cho robust answer matching

**Components ch√≠nh**:

**TextNormalizer Class**

```python
normalizer = TextNormalizer(
    lowercase=True,
    remove_punctuation=True,
    normalize_whitespace=True,
    normalize_numbers=True,      # 1,000 ‚Üí 1000, $1.5K ‚Üí 1500
    remove_accents=True,         # √© ‚Üí e
    fix_common_ocr_errors=True   # O‚Üí0, l‚Üí1 in numeric contexts
)

normalized = normalizer.normalize(text)
```

**Normalization steps**:

1. **Unicode normalization**: NFC form
2. **Remove accents**: `√© ‚Üí e` (NFD decomposition + remove marks)
3. **Fix OCR errors**:
   - `O ‚Üí 0` when surrounded by digits (1O5 ‚Üí 105)
   - `l/I ‚Üí 1` in numeric contexts
   - `| ‚Üí I`, `¬¢ ‚Üí c`, etc.
4. **Normalize numbers**:
   - Remove thousand separators: `1,000 ‚Üí 1000`
   - Expand abbreviations: `1.5K ‚Üí 1500`, `2M ‚Üí 2000000`
   - Remove currency: `$1,234.56 ‚Üí 1234.56`
   - Remove percentage: `50% ‚Üí 50`
5. **Lowercase**: All text
6. **Remove punctuation**: Keep apostrophes/hyphens in words
7. **Normalize whitespace**: Multiple spaces ‚Üí single space

**Example transformations**:

```python
"Invoice Number: INV-2024-001" ‚Üí "invoice number inv 2024 001"
"Total: $1,234.56 (10% tax)"   ‚Üí "total 1234 56 10 tax"
"Date: O3/15/2O24"             ‚Üí "date 03 15 2024"  # OCR fix
"Amount: 1.5K"                 ‚Üí "amount 1500"       # Number normalization
"Caf√© r√©sum√©"                  ‚Üí "cafe resume"       # Accent removal
```

**Fuzzy Matching Utilities**:

```python
# Similarity score (Jaccard on tokens)
similarity = compute_similarity("INV-2024-001", "Invoice: INV-2024-001")
# ‚Üí 0.67 (2/3 tokens match)

# Find answer span in context
start, end, score = find_answer_span(
    answer="INV-2024-001",
    context="Invoice Number: INV-2024-001, Date: 2024-03-15"
)
# ‚Üí (16, 28, 1.0)  # Exact match found
```

**T·∫°i sao c·∫ßn normalization?**

- ‚úÖ **OCR errors**: O/0, l/1/I confusion ph·ªï bi·∫øn
- ‚úÖ **Format variations**: `$1,000` vs `1000` vs `1000.00`
- ‚úÖ **Answer matching**: "March 15" vs "03/15" vs "2024-03-15"
- ‚úÖ **Multi-language**: Accent handling (caf√© vs cafe)
- ‚úÖ **Evaluation**: Fair comparison khi ƒë√°nh gi√° predicted vs ground truth

#### 4. [`pipeline/2_OCR_Extraction.ipynb`](pipeline/2_OCR_Extraction.ipynb)

**M·ª•c ƒë√≠ch**: Demo notebook minh h·ªça full OCR pipeline

**Sections**:

1. **Image Preprocessing**: Before/after visualization
2. **OCR Extraction**: Display tokens v·ªõi confidence
3. **Visualization**: Bounding boxes tr√™n image (color-coded by confidence)
4. **Token Grouping**: Lines v√† blocks visualization
5. **Text Normalization**: Examples + similarity matching
6. **Export**: Save to schema-compliant JSON

**Sample visualizations**:

- Green bbox: confidence > 95%
- Orange bbox: 85-95%
- Red bbox: < 85%

**Output**: `output_ocr_demo.json` - Complete sample theo schema

#### 5. [`src/ocr/__init__.py`](src/ocr/__init__.py)

**M·ª•c ƒë√≠ch**: Package initialization, export public APIs

```python
from ocr import (
    run_ocr,              # Convenience function
    group_tokens,         # Token grouping
    normalize_text,       # Text normalization
    compute_similarity,   # Fuzzy matching
    find_answer_span     # Answer localization
)
```

### Pipeline Flow

```
Image (PNG/JPG)
    ‚Üì
ImagePreprocessor
    ‚îú‚îÄ Resize (300 DPI)
    ‚îú‚îÄ Denoise (Non-Local Means)
    ‚îú‚îÄ Deskew (rotation correction)
    ‚îî‚îÄ Enhance Contrast (CLAHE)
    ‚Üì
PaddleOCR
    ‚îú‚îÄ Text Detection (bounding boxes)
    ‚îú‚îÄ Text Recognition (character sequences)
    ‚îî‚îÄ Direction Classification (rotation)
    ‚Üì
OCR Tokens [x1,y1,x2,y2] + text + confidence
    ‚Üì
TextGrouper
    ‚îú‚îÄ Sort by reading order (Y, X)
    ‚îú‚îÄ Group into Lines (vertical proximity + height similarity)
    ‚îî‚îÄ Group into Blocks (line continuity + horizontal alignment)
    ‚Üì
Lines + Blocks
    ‚Üì
TextNormalizer
    ‚îú‚îÄ Fix OCR errors (O‚Üí0, l‚Üí1)
    ‚îú‚îÄ Normalize numbers (1,000 ‚Üí 1000)
    ‚îú‚îÄ Remove punctuation, accents
    ‚îî‚îÄ Lowercase + whitespace normalization
    ‚Üì
Normalized Text (ready for answer matching)
```

### Usage Examples

**Complete pipeline**:

```python
from ocr import run_ocr, group_tokens, normalize_text

# Step 1: OCR extraction
tokens = run_ocr("invoice.png", lang='en', preprocess=True)

# Step 2: Group tokens
result = group_tokens(tokens)
lines = result['lines']
blocks = result['blocks']

# Step 3: Normalize text
for token in tokens:
    normalized = normalize_text(token['text'], mode='matching')
    print(f"{token['text']} ‚Üí {normalized}")
```

**Answer verification**:

```python
from ocr import compute_similarity

answer = "INV-2024-001"
candidate = "Invoice Number: INV-2024-001"

if compute_similarity(answer, candidate) > 0.8:
    print("‚úÖ Answer matched!")
```

### Output Format (Schema-compliant)

```json
{
  "ocr_data": {
    "engine": "paddleocr",
    "version": "2.7.0",
    "tokens": [...],
    "extraction_time_ms": 1250
  },
  "text_grouping": {
    "lines": [...],
    "blocks": [...]
  }
}
```

### Performance Metrics

**Preprocessing**:

- Resize: ~50ms (2480√ó3508 ‚Üí fit 3000px)
- Denoise: ~200ms (fast NL-Means)
- Deskew: ~100ms (angle detection + rotation)
- Contrast: ~50ms (CLAHE)

**OCR**:

- PaddleOCR (CPU): ~1-2s per page (depends on text density)
- PaddleOCR (GPU): ~300-500ms per page

**Grouping**:

- Line grouping: O(n log n) - sort + linear scan
- Block grouping: O(m) where m = num_lines
- Typical: ~10ms for 100 tokens

**Normalization**:

- Per token: ~1ms (regex operations)

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Or install PaddleOCR separately
pip install paddleocr>=2.7.0

# For GPU support (optional)
pip install paddlepaddle-gpu>=2.5.0
```

### Testing

```bash
# Run demo notebook
jupyter notebook pipeline/2_OCR_Extraction.ipynb

# Or test individual modules
python src/ocr/paddle_ocr_engine.py
python src/ocr/text_grouping.py
python src/ocr/text_normalizer.py
```

---

## ‚úÖ DELIVERABLE 1: Schema Design (HO√ÄN TH√ÄNH)

### T·ªïng quan

ƒê√£ thi·∫øt k·∫ø schema JSON ho√†n ch·ªânh cho 1 sample DocVQA sau khi qua pipeline:

- **OCR extraction**: text + bbox + confidence
- **Layout classification**: TextBlock/Table/Form/Figure/Caption
- **Semantic Graph**: nodes (regions) + edges (spatial relations)
- **QA pairs**: question + answer + evidence + traceability

### Files ƒë√£ t·∫°o

#### 1. [`schema/sample_docvqa_graph.json`](schema/sample_docvqa_graph.json)

**M·ª•c ƒë√≠ch**: Sample data th·ª±c t·∫ø cho 1 invoice document

**N·ªôi dung ch√≠nh**:

```json
{
  "sample_id": "docvqa_00001",
  "image_metadata": {
    "image_id": "img_20260111_001",
    "width": 2480, "height": 3508,
    "document_type": "invoice"
  },
  "ocr_data": {
    "tokens": [
      {"token_id": 0, "text": "INVOICE", "bbox": [120,80,380,140], "confidence": 0.98}
      // ... 27 tokens total
    ]
  },
  "layout_analysis": {
    "regions": [
      {"region_id": "r0", "type": "Title", "text": "INVOICE", "token_indices": [0]},
      {"region_id": "r2", "type": "Table", "text": "Item|Qty|Price|Total...", 
       "table_structure": {"rows": 3, "columns": 4}}
      // ... 6 regions: Title, TextBlock, Table, Form, Figure, Caption
    ]
  },
  "semantic_graph": {
    "nodes": [{"node_id": "n0", "region_id": "r0", "type": "Title"}],
    "edges": [
      {"source": "n0", "target": "n1", "relation": "below", "score": 0.95},
      {"source": "n4", "target": "n5", "relation": "has_caption", "score": 0.98}
      // ... 6 edges v·ªõi spatial features
    ]
  },
  "qa_pairs": [
    {
      "question": "What is the invoice number?",
      "answer": "INV-2024-001",
      "question_type": "simple_lookup",
      "generator": {"type": "rule", "rule_id": "form_field_extraction_v1"},
      "evidence": {
        "region_ids": ["r1"], "token_indices": [3,4,5],
        "bboxes": [[120,250,480,280]]
      }
    },
    {
      "question": "Based on the itemized table and invoice header, how many laptops?",
      "question_type": "cross_element_text_table",
      "generator": {
        "type": "llm", "llm_model": "gpt-4-turbo",
        "temperature": 0.7, "seed": 42
      },
      "evidence": {
        "region_ids": ["r1", "r2"],
        "cross_element_reasoning": {
          "involved_edges": ["e1"],
          "reasoning_chain": ["Identify invoice in header", "Locate table", "Extract qty"]
        }
      }
    }
    // ... 5 QA pairs: easy/medium/hard, rule/LLM, extractive/abstractive
  ]
}
```

**ƒêi·ªÉm ƒë·∫∑c bi·ªát**:

- ‚úÖ 6 lo·∫°i regions (Title, TextBlock, Table, Form, Figure, Caption)
- ‚úÖ Graph v·ªõi 6 edges: spatial (above/below) + semantic (has_caption)
- ‚úÖ 5 QA pairs:
  - 2 rule-based (simple lookup, form-table cross)
  - 3 LLM-based (cross-element reasoning)
- ‚úÖ Evidence tracking: region_ids ‚Üí token_indices ‚Üí bboxes
- ‚úÖ Cross-element reasoning: `involved_edges` + `reasoning_chain`

#### 2. [`schema/schema_definition.md`](schema/schema_definition.md)

**M·ª•c ƒë√≠ch**: Documentation ƒë·∫ßy ƒë·ªß v·ªÅ schema (nh∆∞ TypeScript interfaces)

**N·ªôi dung ch√≠nh**:

**7 Components ch√≠nh**:

1. **Root Structure**: `sample_id`, `version`, `created_at`, 6 components
2. **Image Metadata**: `width`, `height`, `dpi`, `document_type`
3. **OCR Data**:
   - `OCRToken`: `token_id`, `text`, `bbox[x1,y1,x2,y2]`, `confidence`
   - Engine info: `tesseract`, `paddle`, `easyocr`
4. **Layout Analysis**:
   - `LayoutRegion`: `region_id`, `type`, `bbox`, `token_indices`
   - Types: Title/TextBlock/Table/Form/Figure/Caption/Header/Footer/List
   - Type-specific: `table_structure`, `form_fields`, `figure_type`
5. **Semantic Graph**:
   - `GraphNode`: `node_id` ‚Üí `region_id` (1-1 mapping)
   - `GraphEdge`: `source`, `target`, `relation`, `spatial_features`
   - Relations: above/below/left_of/right_of/has_caption/semantic_related
6. **QA Pairs**:
   - `question`, `answer`, `answer_type`, `difficulty`, `question_type`
   - **GeneratorInfo**:
     - Rule: `rule_id`, `template_id`
     - LLM: `prompt_id`, `llm_model`, `temperature`, `seed` ‚úÖ
   - **Evidence**: `region_ids`, `token_indices`, `bboxes`, `cross_element_reasoning`
7. **Metadata**: `dataset_split`, `quality_score`, `processing_pipeline` timestamps

**Key Design Decisions** (5 ƒëi·ªÉm quan tr·ªçng):

1. **Token-level granularity**: Evidence tr·ªè v·ªÅ OCR tokens ‚Üí fine-grained
2. **Graph-centric**: Nodes 1-1 regions, edges c√≥ spatial features ‚Üí GNN models
3. **Dual generation tracking**: Rule vs LLM v·ªõi full traceability
4. **Cross-element metadata**: `question_type` + `involved_edges` + `reasoning_chain`
5. **Extensibility**: Optional fields, version tracking, type-specific metadata

**Usage Examples**:

```python
# OCR-based VQA
tokens = [sample['ocr_data']['tokens'][i] 
          for i in qa['evidence']['token_indices']]

# Graph-augmented VQA
edges = sample['semantic_graph']['edges']
# Build adjacency matrix for GNN

# Cross-element reasoning
edge_ids = qa['evidence']['cross_element_reasoning']['involved_edges']
# Trace reasoning path
```

**Validation Checklist**: 8 checks (bbox ranges, token indices, graph consistency, etc.)

---

## üéØ Schema Design - Gi·∫£i th√≠ch chi ti·∫øt

### T·∫°i sao c·∫ßn schema ph·ª©c t·∫°p?

#### 1. **H·ªó tr·ª£ nhi·ªÅu lo·∫°i VQA models**

- **OCR-based** (LayoutLMv3): c·∫ßn `ocr_tokens` + `bbox`
- **Layout-aware** (Donut): c·∫ßn `regions` + `reading_order`
- **Graph-augmented** (GraphVQA): c·∫ßn `semantic_graph` + `edges`
- **Multi-modal** (mPLUG-DocOwl): c·∫ßn t·∫•t c·∫£ tr√™n + `cross_element_reasoning`

#### 2. **Traceability cho LLM generation**

```json
"generator": {
  "type": "llm",
  "prompt_id": "prompt_cross_element_v2_20260111",  // ‚Üê Prompt version
  "llm_model": "gpt-4-turbo-2024-04-09",            // ‚Üê Model exact version
  "temperature": 0.7,                                // ‚Üê Sampling params
  "seed": 42                                         // ‚Üê Reproducibility
}
```

**L·ª£i √≠ch**:

- Debug: c√¢u h·ªèi n√†o t·ª´ prompt/model n√†o?
- Reproduce: re-run v·ªõi same seed
- A/B test: compare prompt v1 vs v2
- Cost tracking: token usage per model

#### 3. **Evidence cho explainability**

```json
"evidence": {
  "region_ids": ["r1", "r2"],              // ‚Üê Which regions?
  "token_indices": [5, 11],                // ‚Üê Which tokens?
  "bboxes": [[120,250,480,280], ...],      // ‚Üê Where in image?
  "cross_element_reasoning": {
    "involved_edges": ["e1"],              // ‚Üê Which graph edges?
    "reasoning_chain": [                   // ‚Üê Step-by-step logic
      "Identify invoice in header (r1)",
      "Locate table (r2)",
      "Extract quantity"
    ]
  }
}
```

**L·ª£i √≠ch**:

- Visualize: highlight bboxes trong image
- Train v·ªõi attention: supervise attention heads
- Error analysis: wrong answer ‚Üí check evidence path

#### 4. **Cross-element patterns**

```python
question_types = {
  "simple_lookup": 1 region,
  "cross_element_text_table": 2 regions (text + table),
  "cross_element_figure_caption": 2 regions (figure + caption),
  "multi_hop": 3+ regions
}
```

‚Üí Filter/balance dataset theo difficulty

---

## üìä Sample Data Statistics

File `sample_docvqa_graph.json` ch·ª©a:

- **Image**: 2480√ó3508 px invoice
- **OCR**: 28 tokens (INVOICE, Date, items, totals, logo caption)
- **Layout**: 6 regions
  - 1 Title, 1 TextBlock, 1 Table (3√ó4), 1 Form (3 fields), 1 Figure, 1 Caption
- **Graph**: 6 nodes, 6 edges
  - Spatial: above/below/right_of
  - Semantic: has_caption, semantic_related
- **QA**: 5 pairs
  - 2 rule-based (easy/medium)
  - 3 LLM-based (medium/hard)
  - Question types: simple_lookup, cross_element (text-table, form-table, figure-caption, reasoning)

---

## üöÄ Ti·∫øp theo: DELIVERABLE 3-7

### DELIVERABLE 3: Layout Classification

**TODO**:

- [ ] Train/fine-tune layout classifier (LayoutLMv3/YOLO/Faster R-CNN)
- [ ] Region type classification: Title/TextBlock/Table/Form/Figure/Caption
- [ ] Integrate v·ªõi OCR blocks ‚Üí layout regions
- [ ] Script: `src/layout/layout_classifier.py`
- [ ] Output: populate `layout_analysis.regions[]` field

### DELIVERABLE 4: Semantic Graph Construction

**TODO**:

- [ ] Spatial relation heuristics (above/below/left_of/right_of)
- [ ] Semantic relation rules (has_caption, semantic_related)
- [ ] Graph construction: nodes (regions) + edges (relations)
- [ ] Script: `src/graph/graph_builder.py`
- [ ] Output: populate `semantic_graph` field

### DELIVERABLE 5: Hybrid QA Generation

**TODO**:

- [ ] Rule-based templates (form fields, table cells, simple lookup)
- [ ] LLM prompts cho cross-element (GPT-4/Claude)
- [ ] Question type classification (simple/cross-element/multi-hop)
- [ ] Evidence tracking (region_ids, token_spans, bboxes)
- [ ] Scripts:
  - `src/qa_generation/rule_templates.py`
  - `src/qa_generation/llm_prompts.py`
- [ ] Output: populate `qa_pairs[]` v·ªõi generator tracking

### DELIVERABLE 6: Dataset Packaging

**TODO**:

- [ ] Export scripts: JSON ‚Üí JSONL, CSV
- [ ] Evidence pointer validation
- [ ] Train/val/test split (follow DocVQA splits)
- [ ] Dataset statistics report
- [ ] Script: `src/export/package_dataset.py`

### DELIVERABLE 7: Evaluation Framework

**TODO**:

- [ ] Coverage metrics (region types, question types, difficulty)
- [ ] Answerability checks (evidence exists? answer in context?)
- [ ] Consistency validation (answer matches evidence bbox?)
- [ ] Cross-element ratio (% questions using multiple regions)
- [ ] Human spot-check interface
- [ ] Script: `src/evaluation/validate_dataset.py`

---

## üí° S·ª≠ d·ª•ng Schema

### Load sample

```python
import json

with open('schema/sample_docvqa_graph.json') as f:
    sample = json.load(f)

# Access components
image_path = sample['image_metadata']['image_path']
tokens = sample['ocr_data']['tokens']
regions = sample['layout_analysis']['regions']
graph = sample['semantic_graph']
qa_pairs = sample['qa_pairs']
```

### Filter cross-element QA

```python
cross_element_qa = [
    qa for qa in sample['qa_pairs']
    if qa['question_type'].startswith('cross_element')
]

for qa in cross_element_qa:
    print(f"Q: {qa['question']}")
    print(f"Regions: {qa['evidence']['region_ids']}")
    print(f"Edges: {qa['evidence']['cross_element_reasoning']['involved_edges']}")
```

### Visualize graph

```python
import networkx as nx
import matplotlib.pyplot as plt

G = nx.DiGraph()
for node in sample['semantic_graph']['nodes']:
    G.add_node(node['node_id'], type=node['type'])
for edge in sample['semantic_graph']['edges']:
    G.add_edge(edge['source'], edge['target'], 
               relation=edge['relation'], score=edge['score'])

nx.draw(G, with_labels=True)
plt.show()
```

---

## üìù Schema Compliance

Khi implement pipeline, ƒë·∫£m b·∫£o:

‚úÖ **Required fields**: T·∫•t c·∫£ fields kh√¥ng c√≥ `?` trong schema
‚úÖ **Bbox format**: `[x1, y1, x2, y2]` v·ªõi `0 ‚â§ x1 < x2 ‚â§ width`
‚úÖ **Token indices**: Valid indices v√†o `ocr_data.tokens[]`
‚úÖ **Region-node mapping**: M·ªói region c√≥ 1 node trong graph
‚úÖ **Edge validity**: `source`/`target` node_id t·ªìn t·∫°i
‚úÖ **Evidence consistency**: `region_ids`, `token_indices`, `bboxes` align
‚úÖ **Generator tracking**: Rule ph·∫£i c√≥ `rule_id`, LLM ph·∫£i c√≥ `prompt_id` + `model`
‚úÖ **Cross-element**: ‚â•2 `region_ids`, `involved_edges` kh√¥ng empty

---

## üìö T√†i li·ªáu tham kh·∫£o

- **LayoutLMv3**: [Microsoft/unilm](https://github.com/microsoft/unilm/tree/master/layoutlmv3)
- **DocVQA Dataset**: [docvqa.org](https://www.docvqa.org/)
- **Graph Neural Networks**: [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/)

---

## üë• Team & Contact

**Tech Lead**: Your Name
**Role**: Schema design, pipeline architecture
**Date**: January 11, 2026

---

## üìÑ License

[Specify license here]

---

**Version**: 1.0.0
**Last Updated**: 2026-01-11
**Status**: Deliverable 1 ‚úÖ Complete, Deliverables 2-6 üîÑ In Progress
