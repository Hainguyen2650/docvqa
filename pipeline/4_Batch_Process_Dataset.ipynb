{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0252ecb6",
   "metadata": {},
   "source": [
    "# Batch Processing: OCR + Layout + Graph cho to√†n b·ªô DocVQA Dataset\n",
    "\n",
    "Notebook n√†y ch·∫°y to√†n b·ªô pipeline qua dataset v√† l∆∞u k·∫øt qu·∫£:\n",
    "1. **OCR**: PaddleOCR extraction\n",
    "2. **Layout Analysis**: Detect regions (Table, Figure, Form, TextBlock)\n",
    "3. **Graph Building**: Semantic layout graph v·ªõi spatial & semantic relations\n",
    "4. **Export JSON**: L∆∞u to√†n b·ªô v√†o file JSON\n",
    "\n",
    "**Output Format:**\n",
    "```json\n",
    "{\n",
    "  \"image_id\": \"12345\",\n",
    "  \"ocr\": { \"tokens\": [...], \"num_tokens\": 100 },\n",
    "  \"layout\": { \"regions\": [...], \"num_regions\": 5 },\n",
    "  \"graph\": { \"nodes\": [...], \"edges\": [...], \"adjacency\": {...} }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ce4cf",
   "metadata": {},
   "source": [
    "## 1. Import Libraries v√† Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3ecede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported (with reload)!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import pathlib\n",
    "PROJECT_ROOT = pathlib.Path().resolve().parents[1]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "# Force reload modules to get latest changes\n",
    "import importlib\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Import pipeline components\n",
    "from src.ocr.ocr_processor import PaddleOCRProcessor\n",
    "from src.ocr.layout_analyzer import DocumentLayoutAnalyzer\n",
    "from src.graph.graph_builder import GraphBuilder\n",
    "\n",
    "# Import utilities\n",
    "from src.utils import pipeline as pipeline_module\n",
    "from src.utils import batch_processor as batch_module\n",
    "from src.utils import statistics_collector as stats_module\n",
    "\n",
    "# Reload modules to get latest code changes\n",
    "importlib.reload(pipeline_module)\n",
    "importlib.reload(batch_module)\n",
    "importlib.reload(stats_module)\n",
    "\n",
    "from src.utils.pipeline import FullPipelineProcessor\n",
    "from src.utils.batch_processor import BatchProcessor\n",
    "from src.utils.statistics_collector import StatisticsCollector\n",
    "\n",
    "print(\"‚úÖ All libraries imported (with reload)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9ffac72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TokenClassifier loaded!\n",
      "\n",
      "Test v·ªõi sample tokens:\n",
      "  Invoice Date:        -> form_key\n",
      "  12/01/2024           -> form_value\n",
      "  Customer Name:       -> form_key\n",
      "  ABC Corp             -> text\n",
      "\n",
      "Statistics: {'form_key': 2, 'form_value': 1, 'text': 1}\n"
     ]
    }
   ],
   "source": [
    "# Test Token Classifier\n",
    "from src.ocr.token_classifier import TokenClassifier\n",
    "\n",
    "classifier = TokenClassifier()\n",
    "print(\"‚úÖ TokenClassifier loaded!\")\n",
    "print(\"\\nTest v·ªõi sample tokens:\")\n",
    "\n",
    "sample_tokens = [\n",
    "    {'text': 'Invoice Date:', 'bbox': [10, 10, 100, 30]},\n",
    "    {'text': '12/01/2024', 'bbox': [110, 10, 180, 30]},\n",
    "    {'text': 'Customer Name:', 'bbox': [10, 40, 120, 60]},\n",
    "    {'text': 'ABC Corp', 'bbox': [130, 40, 200, 60]},\n",
    "]\n",
    "\n",
    "classified = classifier.classify_tokens(sample_tokens)\n",
    "for token in classified:\n",
    "    print(f\"  {token['text']:20} -> {token['token_type']}\")\n",
    "\n",
    "stats = classifier.get_statistics(classified)\n",
    "print(f\"\\nStatistics: {stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6dce9d",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd4e6f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Dataset Folder: ../dataset/DocVQA_Images\n",
      "üìÅ Output Folder: ../output/full_pipeline\n",
      "üìä Max images per subset: 10\n",
      "üîß Preprocessing: ENABLED\n"
     ]
    }
   ],
   "source": [
    "# Dataset paths\n",
    "IMAGES_FOLDER = Path('../dataset/DocVQA_Images')\n",
    "OUTPUT_FOLDER = Path('../output/full_pipeline')\n",
    "SUBSETS = ['train', 'validation', 'test']\n",
    "\n",
    "# Processing configuration\n",
    "MAX_IMAGES_PER_SUBSET = 10  # None = process all, or set number like 100\n",
    "USE_PREPROCESSING = True\n",
    "MAX_IMAGE_SIZE = 2500\n",
    "\n",
    "# Create output folder\n",
    "OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Dataset Folder:\", IMAGES_FOLDER)\n",
    "print(\"üìÅ Output Folder:\", OUTPUT_FOLDER)\n",
    "print(\"üìä Max images per subset:\", MAX_IMAGES_PER_SUBSET or \"ALL\")\n",
    "print(\"üîß Preprocessing:\", \"ENABLED\" if USE_PREPROCESSING else \"DISABLED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d61e7",
   "metadata": {},
   "source": [
    "## 3. Initialize Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b83f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline with: 10668.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mChecking connectivity to the model hosters, this may take a while. To bypass this check, set `DISABLE_MODEL_SOURCE_CHECK` to `True`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang kh·ªüi t·∫°o PaddleOCR engine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phn/working/.venv/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/phn/.paddlex/official_models/PP-OCRv5_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/phn/.paddlex/official_models/PP-OCRv5_server_rec`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> PaddleOCR ƒë√£ s·∫µn s√†ng!\n",
      "\n",
      "‚úÖ Pipeline test successful!\n",
      "   Tokens: 87\n",
      "   Regions: 4\n",
      "   Edges: 12\n"
     ]
    }
   ],
   "source": [
    "# Test v·ªõi 1 image\n",
    "test_image = IMAGES_FOLDER / 'train' / '10668.png'\n",
    "\n",
    "if test_image.exists():\n",
    "    print(f\"Testing pipeline with: {test_image.name}\")\n",
    "    \n",
    "    # Create temp pipeline (no need to initialize full components yet)\n",
    "    temp_ocr = PaddleOCRProcessor()\n",
    "    temp_layout = DocumentLayoutAnalyzer()\n",
    "    temp_graph = GraphBuilder()\n",
    "    temp_pipeline = FullPipelineProcessor(temp_ocr, temp_layout, temp_graph)\n",
    "    \n",
    "    # Process\n",
    "    result = temp_pipeline.process_image(test_image)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(\"\\n‚úÖ Pipeline test successful!\")\n",
    "        print(f\"   Tokens: {result['num_tokens']}\")\n",
    "        print(f\"   Regions: {result['num_regions']}\")\n",
    "        print(f\"   Edges: {result['num_edges']}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Pipeline test failed: {result.get('error')}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Test image not found: {test_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d09471",
   "metadata": {},
   "source": [
    "## 3.5. Test Pipeline v·ªõi 1 Image (Optional)\n",
    "\n",
    "Test pipeline v·ªõi 1 ·∫£nh tr∆∞·ªõc khi ch·∫°y batch to√†n b·ªô dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc6ce3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/phn/.paddlex/official_models/PP-OCRv5_server_det`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PaddleOCR...\n",
      "ƒêang kh·ªüi t·∫°o PaddleOCR engine...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mCreating model: ('PP-OCRv5_server_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/phn/.paddlex/official_models/PP-OCRv5_server_rec`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> PaddleOCR ƒë√£ s·∫µn s√†ng!\n",
      "Initializing Layout Analyzer...\n",
      "Initializing Graph Builder...\n",
      "Initializing Pipeline Processor...\n",
      "Initializing Batch Processor...\n",
      "\n",
      "‚úÖ All components initialized!\n"
     ]
    }
   ],
   "source": [
    "# Initialize OCR processor\n",
    "print(\"Initializing PaddleOCR...\")\n",
    "ocr_processor = PaddleOCRProcessor(\n",
    "    use_doc_orientation_classify=False,\n",
    "    use_doc_unwarping=False,\n",
    "    use_textline_orientation=False\n",
    ")\n",
    "\n",
    "# Initialize Layout Analyzer\n",
    "print(\"Initializing Layout Analyzer...\")\n",
    "layout_analyzer = DocumentLayoutAnalyzer(\n",
    "    y_overlap_threshold=0.5,\n",
    "    line_height_tolerance=0.3,\n",
    "    max_x_gap_ratio=3.0,\n",
    "    block_vertical_gap=20,\n",
    "    block_x_overlap_threshold=0.3\n",
    ")\n",
    "\n",
    "# Initialize Graph Builder\n",
    "print(\"Initializing Graph Builder...\")\n",
    "graph_builder = GraphBuilder(\n",
    "    iou_threshold=0.1,\n",
    "    distance_threshold=200.0,\n",
    "    projection_threshold=0.3,\n",
    "    max_neighbors=5,\n",
    "    min_edge_score=0.2\n",
    ")\n",
    "\n",
    "# Initialize Full Pipeline Processor\n",
    "print(\"Initializing Pipeline Processor...\")\n",
    "pipeline_processor = FullPipelineProcessor(\n",
    "    ocr_processor=ocr_processor,\n",
    "    layout_analyzer=layout_analyzer,\n",
    "    graph_builder=graph_builder\n",
    ")\n",
    "\n",
    "# Initialize Batch Processor\n",
    "print(\"Initializing Batch Processor...\")\n",
    "batch_processor = BatchProcessor(pipeline_processor)\n",
    "\n",
    "print(\"\\n‚úÖ All components initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a8c4a6",
   "metadata": {},
   "source": [
    "## 4. Run Batch Processing\n",
    "\n",
    "‚ö†Ô∏è **L∆∞u √Ω**: X·ª≠ l√Ω to√†n b·ªô dataset s·∫Ω m·∫•t nhi·ªÅu th·ªùi gian. B·∫Øt ƒë·∫ßu v·ªõi `MAX_IMAGES_PER_SUBSET` nh·ªè ƒë·ªÉ test tr∆∞·ªõc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7d3dd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING BATCH PROCESSING\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Processing subset: TRAIN\n",
      "======================================================================\n",
      "Found 10 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:06<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN Summary:\n",
      "  ‚úÖ Success: 5\n",
      "  ‚ùå Failed: 5\n",
      "  üìä Total: 10\n",
      "\n",
      "======================================================================\n",
      "Processing subset: VALIDATION\n",
      "======================================================================\n",
      "Found 10 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:07<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION Summary:\n",
      "  ‚úÖ Success: 7\n",
      "  ‚ùå Failed: 3\n",
      "  üìä Total: 10\n",
      "\n",
      "======================================================================\n",
      "Processing subset: TEST\n",
      "======================================================================\n",
      "Found 10 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:05<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST Summary:\n",
      "  ‚úÖ Success: 8\n",
      "  ‚ùå Failed: 2\n",
      "  üìä Total: 10\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY\n",
      "======================================================================\n",
      "Total Processed: 30\n",
      "  ‚úÖ Success: 20\n",
      "  ‚ùå Failed: 10\n",
      "Time Elapsed: 0:00:19.483921\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ch·∫°y batch processing\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"STARTING BATCH PROCESSING\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Process dataset using BatchProcessor\n",
    "stats = batch_processor.process_dataset(\n",
    "    images_folder=IMAGES_FOLDER,\n",
    "    output_folder=OUTPUT_FOLDER,\n",
    "    subsets=SUBSETS,\n",
    "    max_images_per_subset=MAX_IMAGES_PER_SUBSET,\n",
    "    skip_existing=True\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "elapsed = end_time - start_time\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total Processed: {stats['total_processed']:,}\")\n",
    "print(f\"  ‚úÖ Success: {stats['total_success']:,}\")\n",
    "print(f\"  ‚ùå Failed: {stats['total_failed']:,}\")\n",
    "print(f\"Time Elapsed: {elapsed}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74bfb5",
   "metadata": {},
   "source": [
    "## 5. Verify Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc14547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "OUTPUT VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "train          : 10 JSON files\n",
      "validation     : 10 JSON files\n",
      "test           : 10 JSON files\n",
      "\n",
      "TOTAL          : 30 JSON files\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count output files\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"OUTPUT VERIFICATION\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "for subset in SUBSETS:\n",
    "    subset_output = OUTPUT_FOLDER / subset\n",
    "    if subset_output.exists():\n",
    "        json_files = list(subset_output.glob('*.json'))\n",
    "        print(f\"{subset:15}: {len(json_files):,} JSON files\")\n",
    "    else:\n",
    "        print(f\"{subset:15}: 0 files (not processed)\")\n",
    "\n",
    "total_files = len(list(OUTPUT_FOLDER.glob('**/*.json')))\n",
    "print(f\"\\n{'TOTAL':15}: {total_files:,} JSON files\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6efae29",
   "metadata": {},
   "source": [
    "## 6. Inspect Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c9748ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Sample file: 1016.json\n",
      "üìÅ Path: ../output/full_pipeline/test/1016.json\n",
      "\n",
      "======================================================================\n",
      "SAMPLE OUTPUT STRUCTURE\n",
      "======================================================================\n",
      "Version: 1.0.0\n",
      "Image ID: 1016\n",
      "\n",
      "OCR:\n",
      "  - Tokens: 28\n",
      "  - Success: True\n",
      "\n",
      "Layout:\n",
      "  - Lines: 24\n",
      "  - Blocks: 11\n",
      "  - Regions: 3\n",
      "\n",
      "Graph:\n",
      "  - Nodes: 3\n",
      "  - Edges: 6\n",
      "\n",
      "Sample Edges (top 5):\n",
      "  1. above (score: 0.810, category: spatial)\n",
      "  2. above (score: 0.643, category: spatial)\n",
      "  3. below (score: 0.810, category: spatial)\n",
      "  4. above (score: 0.657, category: spatial)\n",
      "  5. below (score: 0.657, category: spatial)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load and inspect a sample output file\n",
    "import json\n",
    "\n",
    "sample_files = list(OUTPUT_FOLDER.glob('**/*.json'))\n",
    "sample_files = [f for f in sample_files if f.name != 'dataset_statistics.json']\n",
    "\n",
    "if sample_files:\n",
    "    sample_file = sample_files[0]\n",
    "    print(f\"üìÑ Sample file: {sample_file.name}\")\n",
    "    print(f\"üìÅ Path: {sample_file}\")\n",
    "    \n",
    "    with open(sample_file, 'r', encoding='utf-8') as f:\n",
    "        sample_data = json.load(f)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SAMPLE OUTPUT STRUCTURE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Version: {sample_data['version']}\")\n",
    "    print(f\"Image ID: {sample_data['image_id']}\")\n",
    "    print(f\"\\nOCR:\")\n",
    "    print(f\"  - Tokens: {sample_data['ocr']['num_tokens']}\")\n",
    "    print(f\"  - Success: {sample_data['ocr']['success']}\")\n",
    "    print(f\"\\nLayout:\")\n",
    "    print(f\"  - Lines: {sample_data['layout']['num_lines']}\")\n",
    "    print(f\"  - Blocks: {sample_data['layout']['num_blocks']}\")\n",
    "    print(f\"  - Regions: {sample_data['layout']['num_regions']}\")\n",
    "    print(f\"\\nGraph:\")\n",
    "    print(f\"  - Nodes: {sample_data['graph']['num_nodes']}\")\n",
    "    print(f\"  - Edges: {sample_data['graph']['num_edges']}\")\n",
    "    \n",
    "    if sample_data['graph']['edges']:\n",
    "        print(f\"\\nSample Edges (top 5):\")\n",
    "        for i, edge in enumerate(sample_data['graph']['edges'][:5], 1):\n",
    "            print(f\"  {i}. {edge['relation']} (score: {edge['score']:.3f}, category: {edge.get('category', 'N/A')})\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No output files found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696b4d7",
   "metadata": {},
   "source": [
    "## 7. Collect and Export Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c9d5b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statistics from all processed files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting statistics:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting statistics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 403.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error processing 1027.json: Expecting value: line 8086 column 32 (char 171081)\n",
      "‚ö†Ô∏è Error processing 1028.json: Expecting value: line 8086 column 32 (char 171081)\n",
      "‚ö†Ô∏è Error processing 10002.json: Expecting value: line 3642 column 32 (char 71609)\n",
      "‚ö†Ô∏è Error processing 10004.json: Expecting value: line 3642 column 32 (char 71609)\n",
      "‚ö†Ô∏è Error processing 10013.json: Expecting value: line 8985 column 32 (char 183793)\n",
      "‚ö†Ô∏è Error processing 10015.json: Expecting value: line 8985 column 32 (char 183793)\n",
      "‚ö†Ô∏è Error processing 1002.json: Expecting value: line 5248 column 32 (char 109018)\n",
      "‚ö†Ô∏è Error processing 1023.json: Expecting value: line 6923 column 32 (char 134459)\n",
      "‚ö†Ô∏è Error processing 1024.json: Expecting value: line 6923 column 32 (char 134459)\n",
      "‚ö†Ô∏è Error processing 1025.json: Expecting value: line 6923 column 32 (char 134459)\n",
      "\n",
      "======================================================================\n",
      "DATASET STATISTICS\n",
      "======================================================================\n",
      "Total Images: 30\n",
      "Total OCR Tokens: 781\n",
      "Total Layout Regions: 77\n",
      "Total Graph Edges: 246\n",
      "\n",
      "Averages per Image:\n",
      "  Tokens: 26.0\n",
      "  Regions: 2.6\n",
      "  Edges: 8.2\n",
      "\n",
      "Region Type Distribution:\n",
      "  text           : 60 (77.9%)\n",
      "  form           : 17 (22.1%)\n",
      "\n",
      "Edge Category Distribution:\n",
      "  spatial        : 244 (99.2%)\n",
      "  proximity      : 2 (0.8%)\n",
      "\n",
      "Top 10 Relation Types:\n",
      "  below               : 121 (49.2%)\n",
      "  above               : 119 (48.4%)\n",
      "  left_of             : 2 (0.8%)\n",
      "  right_of            : 2 (0.8%)\n",
      "  nearest_neighbor    : 2 (0.8%)\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Statistics saved to: ../output/full_pipeline/dataset_statistics.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect statistics from all processed files\n",
    "print(\"Collecting statistics from all processed files...\")\n",
    "overall_stats = StatisticsCollector.collect_from_folder(OUTPUT_FOLDER)\n",
    "\n",
    "# Print statistics\n",
    "StatisticsCollector.print_statistics(overall_stats)\n",
    "\n",
    "# Save statistics to JSON\n",
    "stats_file = OUTPUT_FOLDER / 'dataset_statistics.json'\n",
    "StatisticsCollector.save_statistics(overall_stats, stats_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
